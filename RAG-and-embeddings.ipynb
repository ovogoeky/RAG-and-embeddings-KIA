{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a FAISS-Based Vector Store: A Journey Through Data Processing and Visualization\n",
    "\n",
    "In this notebook, you'll learn how to transform raw PDF documents into a searchable vector store using FAISS. We'll go on a journey where we:\n",
    "\n",
    "1. **Read and extract text from PDF files.**\n",
    "2. **Split the text into manageable chunks.**\n",
    "3. **Display tokenization outputs from different tokenizers.**\n",
    "4. **Generate embeddings from the text using a SentenceTransformer.**\n",
    "5. **Store the embeddings in a FAISS index.**\n",
    "6. **Project the embeddings into 2D space using UMAP for visualization.**\n",
    "7. **Visualize the entire process on a scatter plot.**\n",
    "8. **Incect your data into a prompt for a large language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data from PDFs\n",
    "\n",
    "First, we load PDF files from a directory, extract their text content, and combine it into one large text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Z_MB_Merkblatt_Verwendung_von_generativer_KI_in_Arbeiten.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 1/9 [00:00<00:04,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Z_RL_Richtlinie_KI_bei_Leistungsnachweisen.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 2/9 [00:00<00:03,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Bibliotheksangebot_Bachelorarbeit_HS24FS25.pdf'>\n",
      "<_io.BufferedReader name='data/ZHAW_Zitierleitfaden_DE.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:01<00:02,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Z_RL_Richtlinie_Anhang_Deklarationspflicht_KI_bei_Arbeiten.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:02<00:01,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/05_Checkliste_Sprachliche_Formale_Ausarbeitung.pdf'>\n",
      "<_io.BufferedReader name='data/Schwerpunktthemen_fuer_Studenten.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:02<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/02_Merkblatt_Vermeidung-von-Plagiaten_0916.pdf'>\n",
      "<_io.BufferedReader name='data/W_MB_Merkblatt_Bachelorarbeit_BSc.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:03<00:00,  2.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Z-MB-Merkblatt Verwendung von  \\ngenerativer KI bei'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        print(file)\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the Text into Chunks\n",
    "\n",
    "Large texts can be difficult to work with. We use a text splitter, in this case [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/),  to break the full text into smaller, overlapping chunks. This helps preserve context when we later embed the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter: 2000 characters per chunk with an overlap of 200 characters\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 62\n",
      "Preview of the first chunk: Z-MB-Merkblatt Verwendung von  \n",
      "generativer KI bei Arbeiten  \n",
      "Version:  1.2.0 gÃ¼ltig ab:  01.03.2025   Seite 1 von 5 \n",
      " Rektorat  \n",
      "Ressort Bildung  \n",
      "Verwendung von generativer KI bei Arbeiten  \n",
      "Dieses \n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Text with Different Tokenizers\n",
    "\n",
    "Before embedding, it's insightful to see how different tokenizers break up our text. Here, we use the tokenizer from the SentenceTransformer model (see [SentenceTransformersTokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html#sentencetransformerstokentextsplitter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 254\n",
      "Z-MB-Merkblatt Verwendung von generativer KI bei Arbeiten Version: 1.2.0 gÃ¼ltig ab: 01.03.2025 Seite 1 von 5 Rektorat Ressort Bildung Verwendung von generativer KI bei Arbeiten Dieses Merkblatt basiert auf : âˆ’ Richtlinie KI bei Leistungsnachweisen âˆ’ ergÃ¤nzender Anhang 1. Grundsatz GrundsÃ¤tzlich gilt, dass die Verwendung von generativen KI -Tools bei Arbeiten zu de- klarieren ist. Dieses Merkblatt hilft Ihnen Schritt -fÃ¼r-Schritt bei der korrekten Umset- zung. 2. Pflichten im Umgang Ihre Pflichten im Umgang mit generativer KI âˆ’ Sie\n"
     ]
    }
   ],
   "source": [
    "token_split_texts = []\n",
    "for text in chunks:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")\n",
    "print(token_split_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['<s>', 'â–Z', '-', 'MB', '-', 'Mer', 'k', 'blatt', 'â–Verwendung', 'â–von', 'â–genera', 'ti', 'ver', 'â–KI', 'â–bei', 'â–Arbeiten', 'â–Version', ':', 'â–1.', '2.0', 'â–gÃ¼ltig', 'â–ab', ':', 'â–01.', '03.20', '25', 'â–Seite', 'â–1', 'â–von', 'â–5', 'â–Rektor', 'at', 'â–Res', 'sort', 'â–Bildung', 'â–Verwendung', 'â–von', 'â–genera', 'ti', 'ver', 'â–KI', 'â–bei', 'â–Arbeiten', 'â–Dieses', 'â–Merk', 'blatt', 'â–bas', 'iert', 'â–auf', 'â–:', 'â–âˆ’', 'â–', 'Richtlinie', 'â–KI', 'â–bei', 'â–Leistungs', 'nach', 'weise', 'n', 'â–âˆ’', 'â–ergÃ¤nzen', 'der', 'â–An', 'hang', 'â–1.', 'â–Grund', 'satz', 'â–GrundsÃ¤tzlich', 'â–gilt', ',', 'â–dass', 'â–die', 'â–Verwendung', 'â–von', 'â–genera', 'tive', 'n', 'â–KI', 'â–-', 'Tool', 's', 'â–bei', 'â–Arbeiten', 'â–zu', 'â–de', '-', 'â–klar', 'ieren', 'â–ist', '.', 'â–Dieses', 'â–Merk', 'blatt', 'â–hilft', 'â–Ihnen', 'â–Schritt', 'â–-', 'fÃ¼r', '-', 'Sch', 'rit', 't', 'â–bei', 'â–der', 'â–korrekt', 'en', 'â–Um', 'set', '-', 'â–', 'zung', '.', 'â–2.', 'â–Pflicht', 'en', 'â–im', 'â–Umgang', 'â–Ihre', 'â–Pflicht', 'en', 'â–im', 'â–Umgang', 'â–mit', 'â–genera', 'ti', 'ver', 'â–KI', '</s>']\n",
      "Chunk 1: ['<s>', 'â–tragen', 'â–Verantwortung', 'â–fÃ¼r', 'â–die', 'â–QualitÃ¤t', 'â–der', 'â–Inhalte', 'â–:', 'â–Auch', 'â–bei', 'â–korrekt', 'er', 'â–Deklar', 'ation', 'â–bleiben', 'â–Sie', 'â–verpflichtet', ',', 'â–die', 'â–Re', 'leva', 'nz', ',', 'â–den', 'â–Wahrheit', 's', 'ge', 'halt', 'â–oder', 'â–die', 'â–Genau', 'igkeit', 'â–sowohl', 'â–der', 'â–eingesetzt', 'en', 'â–genera', 'tive', 'n', 'â–KI', 'â–-', 'Tool', 's', 'â–als', 'â–auch', 'â–des', 'â–Out', 'put', 's', 'â–zu', 'â–Ã¼berprÃ¼fen', '.', 'â–âˆ’', 'â–Die', 'â–Arbeit', 'â–muss', 'â–Ihre', 'â–eigen', 'stÃ¤ndig', 'e', 'â–Leistung', 'â–sein', 'â–:', 'â–Genera', 'tive', 'â–KI', 'â–darf', 'â–aus', 'sch', 'lies', 's', 'lich', 'â–unterstÃ¼tzen', 'd', 'â–eingesetzt', 'â–werden', '.', 'â–Der', 'â–Anteil', 'â–bzw', '.', 'â–das', 'â–Aus', 'mas', 's', 'â–des', 'â–Mit', 'wirken', 's', 'â–von', 'â–genera', 'tive', 'n', 'â–KI', 'â–-', 'Tool', 's', 'â–an', 'â–der', 'â–sch', 'Ã¶pfe', 'r', 'ischen', 'â–Leistung', 'â–Ihrer', 'â–Arbeit', 'â–muss', 'â–fÃ¼r', 'â–Dritte', 'â–erken', 'n', 'bar', 'â–sein', '.', 'â–âˆ’', 'â–Sie', 'â–dÃ¼rfen', 'â–nicht', 'â–jede', 'â–Art', 'â–von', 'â–Daten', 'â–mit', 'â–genera', 'tive', 'n', '</s>']\n",
      "Chunk 2: ['<s>', 'â–System', 'en', 'â–be', 'arbeiten', 'â–:', 'â–GrundsÃ¤tzlich', 'â–ist', 'â–die', 'â–Bearbeitung', 'â–von', 'â–folgenden', 'â–Art', 'â–Daten', 'â–nicht', 'â–erlaubt', ':', 'â–besonders', 'â–sch', 'Ã¼t', '-', 'â–zen', 's', 'werte', 'â–Per', 's', 'â–on', 'en', 'daten', 'â–(', 'z', '.', 'B', '.', 'â–Daten', 'â–Ã¼ber', 'â–religi', 'Ã¶s', 'e', 'â–Ansicht', 'en', ',', 'â–Gesundheit', ',', 'â–oder', 'â–gene', 'ti', '-', 'â–', 'sche', 'â–Daten', ');', 'â–Betriebs', 'â–-', 'â–und', 'â–GeschÃ¤fts', 'ge', 'heim', 'nisse', 'â–(', 'jur', 'istische', 'r', 'â–Personen', ').', 'â–Z', '-', 'MB', '-', 'Mer', 'k', 'blatt', 'â–Verwendung', 'â–von', 'â–genera', 'ti', 'ver', 'â–KI', 'â–bei', 'â–Arbeiten', 'â–Version', ':', 'â–1.', '2.0', 'â–gÃ¼ltig', 'â–ab', ':', 'â–01.', '03.20', '25', 'â–Seite', 'â–2', 'â–von', 'â–5', 'â–Rektor', 'at', 'â–Res', 'sort', 'â–Bildung', 'â–3.', 'â–Ãœbersicht', ':', 'â–Schritt', 'â–fÃ¼r', 'â–Schritt', 'â–zur', 'â–korrekt', 'en', 'â–Deklar', 'ation', 'â–Diese', 'â–Angaben', 'â–dienen', 'â–der', 'â–Orient', 'ierung', '.', 'â–Im', 'â–Zweifel', 's', 'fall', 'â–konsult', 'ieren', 'â–Sie', 'â–bitte', 'â–den', 'â–Rich', '</s>']\n",
      "Chunk 3: ['<s>', 'â–nie', 'n', '-', 'An', 'hang', 'â–Deklar', 'a', 'â–', 'tions', 'pflicht', 'â–von', 'â–genera', 'ti', 'ver', 'â–KI', 'â–bei', 'â–Arbeiten', 'â–oder', 'â–kontaktieren', 'â–Ihre', 'â–Do', '-', 'â–zen', 'tin', 'â–oder', 'â–Ihren', 'â–Do', 'zen', 'ten', 'â–', '.', 'â–Hat', 'â–Ihr', 'â–Studien', 'gang', 'â–oder', 'â–Do', 'zen', 't', ':', 'in', 'â–fÃ¼r', 'â–Ihre', 'â–Arbeit', 'â–Regeln', 'â–', 'bezÃ¼glich', 'â–Einsatz', 'â–genera', 'ti', 'ver', 'â–KI', 'â–-', 'Tool', 's', 'â–festgelegt', '?', 'â–Ja', 'â–Es', 'â–gelten', 'â–die', '</s>']\n",
      "Chunk 4: ['<s>', 'â–zen', 'tin', 'â–oder', 'â–Ihren', 'â–Do', 'zen', 'ten', 'â–', '.', 'â–Hat', 'â–Ihr', 'â–Studien', 'gang', 'â–oder', 'â–Do', 'zen', 't', ':', 'in', 'â–fÃ¼r', 'â–Ihre', 'â–Arbeit', 'â–Regeln', 'â–', 'bezÃ¼glich', 'â–Einsatz', 'â–genera', 'ti', 'ver', 'â–KI', 'â–-', 'Tool', 's', 'â–festgelegt', '?', 'â–Ja', 'â–Es', 'â–gelten', 'â–die', 'â–An', 'weis', 'ungen', 'â–Ihrer', 'â–Do', 'zen', 'tin', 'â–oder', 'â–Ihres', 'â–Do', 'zen', 'ten', '.', 'We', 'is', 's', 'â–nicht', 'â–Inform', 'ieren', 'â–Sie', 'â–sich', 'â–bei', 'â–Ihrer', 'â–Do', 'zen', 'tin', 'â–oder', 'â–Ihrem', 'â–Do', 'zen', 'ten', '.', 'Ne', 'in', 'â–Haben', 'â–Sie', 'â–bei', 'â–der', 'â–Er', 'stellung', 'â–Ihrer', 'â–Arbeit', 'â–genera', 'tive', 'â–KI', 'â–-', 'Tool', 's', 'â–verwendet', '?', 'â–Nein', 'â–â€”', 'â–â€”', 'â–â€”', 'Ja', ',', 'â–-', 'als', 'â–Formul', 'ierung', 's', 'hilfe', 'â–-', 'als', 'â–Rechts', 'schreib', 'e', 'hilfe', 'â–-', 'zur', 'â–Sprache', 'rg', 'Ã¤n', 'zung', 'â–â€”', 'â–â€”', 'â–â€”', '-', 'als', 'â–Gedanken', 'an', 'stos', 's', 'â–-', 'als', 'â–Inspiration', 'sque', '</s>']\n",
      "Chunk 5: ['<s>', 'â–â€”', 'â–âœ“', '-', 'Out', 'put', 'â–', 'wort', 'w', 'Ã¶rt', 'lich', 'â–oder', 'â–sinn', 'ge', 'mÃ¤', 's', 's', 'â–-', 'zur', 'â–Bilder', 'stellung', 'â–âœ“', 'â–âœ“', 'â–âœ“', 'ander', 'weit', 'ig', 'â–zur', 'â–-', 'Text', 'er', 'stellung', 'â–-', '(', 'We', 'iter', 'â–-', ')', 'Be', 'arbeit', 'ung', 'â–â€”', 'â–âœ“', 'â–âœ“', 'â–VER', 'ZE', 'ICH', 'NIS', 'â–-', 'RE', 'F', 'EREN', 'Z', 'â–KI', '-', 'Tool', 's', 'â–und', 'â–Verwendung', 's', 'zweck', 'â–mÃ¼ssen', 'â–auf', 'gefÃ¼hrt', 'â–werden', 'â–', 'MET', 'HO', 'DEN', 'â–-', 'RE', 'F', 'EREN', 'Z', 'â–KI', '-', 'Nu', 't', 'zung', 'â–muss', 'â–beschrieben', 'â–und', 'â–reflekt', 'iert', 'â–werden', 'â–IN', '-', 'TEX', 'T', 'â–-', 'RE', 'F', 'EREN', 'Z', 'â–KI', '-', 'Out', 'put', 'â–muss', 'â–i', 'â–m', 'â–Text', 'â–korrekt', 'â–zit', 'iert', 'â–werden', 'â–Deklar', 'a', 'â–tion', 'â–als', 'â–...', 'â–Z', '-', 'MB', '-', 'Mer', 'k', 'blatt', 'â–Verwendung', 'â–von', 'â–genera', 'ti', 'ver', 'â–KI', 'â–bei', 'â–Arbeiten', 'â–Version', '</s>']\n",
      "Chunk 6: ['<s>', 'â–2.0', 'â–gÃ¼ltig', 'â–ab', ':', 'â–01.', '03.20', '25', 'â–Seite', 'â–3', 'â–von', 'â–5', 'â–Rektor', 'at', 'â–Res', 'sort', 'â–Bildung', 'â–4.', 'â–In', '-', 'Text', '-', 'Ref', 'eren', 'z', ':', 'â–Out', 'put', 'â–im', 'â–Text', 'â–zit', 'ieren', 'â–Sie', 'â–mÃ¼ssen', 'â–Out', 'put', 'â–von', 'â–genera', 'tive', 'n', 'â–KI', 'â–-', 'Tool', 's', 'â–', ',', 'â–welchen', 'â–Sie', 'â–', 'wort', 'w', 'Ã¶rt', 'lich', 'â–oder', 'â–para', 'phra', '-', 'â–si', 'erend', 'â–in', 'â–Ihrer', 'â–Arbeit', 'â–verwenden', 'â–', ',', 'â–zit', 'ieren', '.', 'â–Dies', 'â–gilt', 'â–auch', 'â–fÃ¼r', 'â–den', 'â–Fall', ',', 'â–dass', 'â–Sie', 'â–diesen', 'â–nur', 'â–auszu', 'g', 'sweise', 'â–oder', 'â–sinn', 'ge', 'mÃ¤', 's', 's', 'â–Ã¼bernehmen', '.', 'â–4.1', 'â–Zi', 'tier', 'vor', 'gabe', 'â–4.2', 'â–Beispiel', 'e', 'â–von', 'â–Kurz', 'v', 'erweise', 'n', 'â–im', 'â–Text', 'â–Art', 'â–der', 'â–Nutzung', 'â–Beispiel', 'â–W', 'Ã¶rt', 'liche', 'â–Ãœber', 'nahme', 'â–Â«', 'Bild', 'ungs', 'management', 'â–findet', 'â–auf', 'â–verschiedenen', 'â–Ebene', 'n', 'â–statt', ',', 'â–von', 'â–der', '</s>']\n",
      "Chunk 7: ['<s>', 'â–r', 'â–Bildungs', 'ein', 'richtung', 'en', 'â–wie', 'â–Schu', '-', 'â–len', ',', 'â–Hochschule', 'n', 'â–oder', 'â–Weiterbildung', 'szent', 'ren', 'â–bis', 'â–hin', 'â–zur', 'â–Ste', 'u', '-', 'â–', 'erung', 'â–ganz', 'er', 'â–Bildungs', 'system', 'e', 'â–Â»', 'â–(', 'Open', 'AI', ',', 'â–202', 'â–4)', '.', 'â–Para', 'phra', 's', 'ierung', 'â–Bildungs', 'management', 'â–erfolgt', 'â–auf', 'â–unterschiedlich', 'en', 'â–Ebene', 'n', ',', 'â–von', 'â–der', 'â–Ad', 'â–ministra', 'tion', 'â–individuelle', 'r', 'â–Bildungs', 'institution', 'en', 'â–bis', 'â–hin', 'â–zur', 'â–Len', 'kung', 'â–ganz', 'er', 'â–Bildungs', 'system', 'e', 'â–(', 'Open', 'AI', ',', 'â–202', 'â–4)', '.', 'â–Mas', 'chi', 'nelle', 'â–Sprach', '-', 'â–Ã¼ber', 'setzung', 'â–Â«', 'La', 'â–gestion', 'â–de', 'â–l', \"'\", 'Ã©ducation', 'â–se', 'â–fait', 'â–Ã ', 'â–diffÃ©rents', 'â–niveaux', ',', 'â–de', 'â–l', \"'\", 'ad', '-', '</s>']\n",
      "Chunk 8: ['<s>', 'â–hin', 'â–zur', 'â–Len', 'kung', 'â–ganz', 'er', 'â–Bildungs', 'system', 'e', 'â–(', 'Open', 'AI', ',', 'â–202', 'â–4)', '.', 'â–Mas', 'chi', 'nelle', 'â–Sprach', '-', 'â–Ã¼ber', 'setzung', 'â–Â«', 'La', 'â–gestion', 'â–de', 'â–l', \"'\", 'Ã©ducation', 'â–se', 'â–fait', 'â–Ã ', 'â–diffÃ©rents', 'â–niveaux', ',', 'â–de', 'â–l', \"'\", 'ad', '-', 'â–ministra', 'tion', 'â–d', \"'\", 'Ã©tablissement', 's', 'â–d', \"'\", 'enseignement', 'â–individuel', 's', 'â–tel', 's', 'â–que', 'â–les', 'â–Ã©coles', ',', 'â–les', 'â–', 'universitÃ©', 's', 'â–ou', 'â–les', 'â–centres', 'â–de', 'â–formation', 'â–con', '-', 'â–tin', 'ue', 'â–Ã ', 'â–la', 'â–gestion', 'â–de', 'â–systÃ¨me', 's', 'â–Ã©', 'duc', 'atif', 's', 'â–complet', 's', 'â–Â»', 'â–(', 'De', 'ep', 'L', ',', 'â–20', '24)', '.', 'â–Bild', 'â–inkl', '.', 'â–Pro', 'mp', 't', 'â–Â«', 'Sy', 'mbo', 'lik', 'â–fÃ¼r', 'â–Bildungs', 'management', 'â–Â»', 'â–(', 'D', 'ALL', 'â–-', 'E', 'â–2', ',', 'â–Open', 'AI', ',', 'â–202', 'â–4)', 'â–Zu', 'rÃ¼ck', 'â–zur', 'â–Ãœbersicht', 'â–â†‘', 'â–âˆ’', 'â–Autor', 'â–des', '</s>']\n",
      "Chunk 9: ['<s>', 'â–:', 'â–z', '.', 'B', '.', 'â–Open', 'AI', 'â–als', 'â–Autor', 'â–des', 'â–Large', 'â–Language', 'â–Model', 's', 'â–Chat', 'G', 'PT', 'â–âˆ’', 'â–Datum', 'â–:', 'â–das', 'â–Jahr', 'â–in', 'â–welche', 'm', 'â–ein', 'â–KI', 'â–-', 'Tool', 'â–genutzt', 'â–wurde', 'â–âˆ’', 'â–Bei', 'â–Bilder', 'n', 'â–Bild', 'be', 'zeichnung', 'â–mit', 'â–In', 'iti', 'al', 'pro', 'mp', 't', 'â–:', 'â–in', 'â–An', 'fÃ¼hrung', 's', 'â–-', 'â–und', 'â–Schluss', 'ze', 'i', '-', 'â–che', 'n', 'â–Zi', 'tier', 'vor', 'gaben', 'â–Z', '-', 'MB', '-', 'Mer', 'k', 'blatt', 'â–Verwendung', 'â–von', 'â–genera', 'ti', 'ver', 'â–KI', 'â–bei', 'â–Arbeiten', 'â–Version', ':', 'â–1.', '2.0', 'â–gÃ¼ltig', 'â–ab', ':', 'â–01.', '03.20', '25', 'â–Seite', 'â–4', 'â–von', 'â–5', 'â–Rektor', 'at', 'â–Res', 'sort', 'â–Bildung', 'â–5.', 'â–Ver', 'ze', 'ich', 'nis', 'â–-', 'Ref', 'eren', 'z', ':', 'â–Hilfs', 'mittel', 'ver', 'ze', 'ich', 'nis', 'â–inkl', '.', 'â–Verwendung', 's', 'zweck', 'â–Sie', 'â–mÃ¼ssen', 'â–v', 'â–er', 'wende', 'te', 'â–genera', 'tive', 'â–KI', '</s>']\n"
     ]
    }
   ],
   "source": [
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['[CLS]', 'Z', '-', 'MB', '-', 'Merk', '##blatt', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Version', ':', '1', '.', '2', '.', '0', 'gÃ¼ltig', 'ab', ':', '01', '.', '03', '.', '202', '##5', 'Seite', '1', 'von', '5', 'Rektor', '##at', 'Ressort', 'Bildung', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Dieses', 'Merk', '##blatt', 'basiert', 'auf', ':', 'âˆ’', 'Richtlinie', 'K', '##I', 'bei', 'Leistungs', '##nach', '##weisen', 'âˆ’', 'ergÃ¤nzen', '##der', 'Anhang', '1', '.', 'Grundsatz', 'GrundsÃ¤tzlich', 'gilt', ',', 'dass', 'die', 'Verwendung', 'von', 'gener', '##ativen', 'K', '##I', '-', 'Tool', '##s', 'bei', 'Arbeiten', 'zu', 'de', '-', 'klar', '##ieren', 'ist', '.', 'Dieses', 'Merk', '##blatt', 'hilft', 'Ihnen', 'Schritt', '-', 'fÃ¼r', '-', 'Schritt', 'bei', 'der', 'korrekt', '##en', 'Um', '##set', '-', 'zun', '##g', '.', '2', '.', 'Pflichten', 'im', 'Umgang', 'Ihre', 'Pflichten', 'im', 'Umgang', 'mit', 'gener', '##ativer', 'K', '##I', 'âˆ’', 'Sie', '[SEP]']\n",
      "Chunk 1: ['[CLS]', 'tragen', 'Verantwortung', 'fÃ¼r', 'die', 'QualitÃ¤t', 'der', 'Inhalte', ':', 'Auch', 'bei', 'korrekt', '##er', 'Dek', '##lar', '##ation', 'bleiben', 'Sie', 'verpflichtet', ',', 'die', 'Relevanz', ',', 'den', 'Wahrheit', '##sg', '##eha', '##lt', 'oder', 'die', 'Genauigkeit', 'sowohl', 'der', 'eingesetzten', 'gener', '##ativen', 'K', '##I', '-', 'Tool', '##s', 'als', 'auch', 'des', 'Out', '##put', '##s', 'zu', 'Ã¼berprÃ¼fen', '.', 'âˆ’', 'Die', 'Arbeit', 'muss', 'Ihre', 'eigenstÃ¤ndige', 'Leistung', 'sein', ':', 'Gener', '##ative', 'K', '##I', 'darf', 'aussch', '##liesslich', 'unterstÃ¼tzen', '##d', 'eingesetzt', 'werden', '.', 'Der', 'Anteil', 'bzw', '.', 'das', 'Aus', '##mass', 'des', 'Mit', '##wirken', '##s', 'von', 'gener', '##ativen', 'K', '##I', '-', 'Tool', '##s', 'an', 'der', 'sch', '##Ã¶pfe', '##rischen', 'Leistung', 'Ihrer', 'Arbeit', 'muss', 'fÃ¼r', 'Dritte', 'erkennbar', 'sein', '.', 'âˆ’', 'Sie', 'dÃ¼rfen', 'nicht', 'jede', 'Art', 'von', 'Daten', 'mit', 'gener', '##ativen', 'K', '##I', '-', '[SEP]']\n",
      "Chunk 2: ['[CLS]', 'Systemen', 'bearbeiten', ':', 'GrundsÃ¤tzlich', 'ist', 'die', 'Bearbeitung', 'von', 'folgenden', 'Art', 'Daten', 'nicht', 'erlaubt', ':', 'besonders', 'sch', '##Ã¼t', '-', 'z', '##ens', '##werte', 'Pers', 'one', '##nd', '##aten', '(', 'z', '.', 'B', '.', 'Daten', 'Ã¼ber', 'religiÃ¶se', 'Ansichten', ',', 'Gesundheit', ',', 'oder', 'gen', '##eti', '-', 'sch', '##e', 'Daten', ')', ';', 'Betriebs', '-', 'und', 'GeschÃ¤fts', '##geheim', '##nisse', '(', 'juristische', '##r', 'Personen', ')', '.', 'Z', '-', 'MB', '-', 'Merk', '##blatt', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Version', ':', '1', '.', '2', '.', '0', 'gÃ¼ltig', 'ab', ':', '01', '.', '03', '.', '202', '##5', 'Seite', '2', 'von', '5', 'Rektor', '##at', 'Ressort', 'Bildung', '3', '.', 'Ãœbersicht', ':', 'Schritt', 'fÃ¼r', 'Schritt', 'zur', 'korrekt', '##en', 'Dek', '##lar', '##ation', 'Diese', 'Angaben', 'dienen', 'der', 'Orientierung', '.', 'Im', 'Zweifel', '##sf', '##all', 'konsul', '##tieren', 'Sie', 'bitte', 'den', 'Richt', '##li', '[SEP]']\n",
      "Chunk 3: ['[CLS]', 'nie', '##n', '-', 'Anhang', 'Dek', '##lar', '##a', 'ti', '##ons', '##pflicht', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'oder', 'kontaktieren', 'Ihre', 'Do', '-', 'z', '##entin', 'oder', 'Ihren', 'Dozent', '##en', '.', 'Hat', 'Ihr', 'Studien', '##gang', 'oder', 'Dozent', ':', 'in', 'fÃ¼r', 'Ihre', 'Arbeit', 'Regeln', 'bezÃ¼glich', 'Einsatz', 'gener', '##ativer', 'K', '##I', '-', 'Tool', '##s', 'festgelegt', '?', 'Ja', 'Es', 'gelten', 'die', '[SEP]']\n",
      "Chunk 4: ['[CLS]', 'z', '##entin', 'oder', 'Ihren', 'Dozent', '##en', '.', 'Hat', 'Ihr', 'Studien', '##gang', 'oder', 'Dozent', ':', 'in', 'fÃ¼r', 'Ihre', 'Arbeit', 'Regeln', 'bezÃ¼glich', 'Einsatz', 'gener', '##ativer', 'K', '##I', '-', 'Tool', '##s', 'festgelegt', '?', 'Ja', 'Es', 'gelten', 'die', 'Anweisungen', 'Ihrer', 'Dozent', '##in', 'oder', 'Ihres', 'Dozent', '##en', '.', 'Weiss', 'nicht', 'Inform', '##ieren', 'Sie', 'sich', 'bei', 'Ihrer', 'Dozent', '##in', 'oder', 'Ihrem', 'Dozent', '##en', '.', 'Nein', 'Haben', 'Sie', 'bei', 'der', 'Erstellung', 'Ihrer', 'Arbeit', 'gener', '##ative', 'K', '##I', '-', 'Tool', '##s', 'verwendet', '?', 'Nein', 'â€”', 'â€”', 'â€”', 'Ja', ',', '-', 'als', 'Formulierung', '##shi', '##lf', '##e', '-', 'als', 'Rechts', '##sch', '##reibe', '##hilfe', '-', 'zur', 'Sprache', '##rg', '##Ã¤n', '##zung', 'â€”', 'â€”', 'â€”', '-', 'als', 'Gedanken', '##anst', '##oss', '-', 'als', 'Inspiration', '##s', '##quelle', 'â€”', '[SEP]']\n",
      "Chunk 5: ['[CLS]', 'â€”', '[UNK]', '-', 'Out', '##put', 'wor', '##tw', '##Ã¶rtlich', 'oder', 'sinn', '##gemÃ¤', '##ss', '-', 'zur', 'Bilder', '##stellung', '[UNK]', '[UNK]', '[UNK]', 'zur', '-', 'Texte', '##rs', '##tell', '##ung', '-', '(', 'Weiter', '-', ')', 'Bearbeitung', 'â€”', '[UNK]', '[UNK]', 'VER', '##ZE', '##ICH', '##NIS', '-', 'RE', '##FE', '##R', '##EN', '##Z', 'K', '##I', '-', 'Tool', '##s', 'und', 'Verwendungs', '##zweck', 'mÃ¼ssen', 'aufgefÃ¼hrt', 'werden', 'M', '##ET', '##HO', '##DEN', '-', 'RE', '##FE', '##R', '##EN', '##Z', 'K', '##I', '-', 'Nutzung', 'muss', 'beschrieben', 'und', 'refle', '##ktiert', 'werden', 'IN', '-', 'T', '##EX', '##T', '-', 'RE', '##FE', '##R', '##EN', '##Z', 'K', '##I', '-', 'Out', '##put', 'muss', 'i', 'm', 'Text', 'korrekt', 'zitiert', 'werden', 'Dek', '##lar', '##a', 'ti', '##on', 'als', '.', '.', '.', 'Z', '-', 'MB', '-', 'Merk', '##blatt', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Version', ':', '1', '.', '[SEP]']\n",
      "Chunk 6: ['[CLS]', '2', '.', '0', 'gÃ¼ltig', 'ab', ':', '01', '.', '03', '.', '202', '##5', 'Seite', '3', 'von', '5', 'Rektor', '##at', 'Ressort', 'Bildung', '4', '.', 'In', '-', 'Text', '-', 'Referenz', ':', 'Out', '##put', 'im', 'Text', 'zit', '##ieren', 'Sie', 'mÃ¼ssen', 'Out', '##put', 'von', 'gener', '##ativen', 'K', '##I', '-', 'Tool', '##s', ',', 'welchen', 'Sie', 'wor', '##tw', '##Ã¶rtlich', 'oder', 'par', '##aph', '##ra', '-', 'sie', '##ren', '##d', 'in', 'Ihrer', 'Arbeit', 'verwenden', ',', 'zit', '##ieren', '.', 'Dies', 'gilt', 'auch', 'fÃ¼r', 'den', 'Fall', ',', 'dass', 'Sie', 'diesen', 'nur', 'auszu', '##gs', '##weise', 'oder', 'sinn', '##gemÃ¤', '##ss', 'Ã¼bernehmen', '.', '4', '.', '1', 'Zit', '##ier', '##vor', '##gabe', '4', '.', '2', 'Beispiele', 'von', 'Kurz', '##verw', '##eisen', 'im', 'Text', 'Art', 'der', 'Nutzung', 'Beispiel', 'WÃ¶r', '##tliche', 'Ãœbernahme', 'Â«', 'Bildungs', '##management', 'findet', 'auf', 'verschiedenen', 'Ebenen', 'statt', ',', 'von', 'der', 'Verwaltung', 'einzelne', '[SEP]']\n",
      "Chunk 7: ['[CLS]', 'r', 'Bildungs', '##einrichtungen', 'wie', 'Schu', '-', 'len', ',', 'Hochschulen', 'oder', 'Weiterbildungs', '##zentren', 'bis', 'hin', 'zur', 'Ste', '##u', '-', 'er', '##ung', 'ganze', '##r', 'Bildungs', '##systeme', 'Â»', '(', 'Open', '##A', '##I', ',', '202', '4', ')', '.', 'Par', '##aph', '##ras', '##ierung', 'Bildungs', '##management', 'erfolgt', 'auf', 'unterschiedlich', '##en', 'Ebenen', ',', 'von', 'der', 'Ad', 'min', '##istr', '##ation', 'individuelle', '##r', 'Bildungs', '##institutionen', 'bis', 'hin', 'zur', 'Lenk', '##ung', 'ganze', '##r', 'Bildungs', '##systeme', '(', 'Open', '##A', '##I', ',', '202', '4', ')', '.', 'Maschine', '##lle', 'Sprach', '-', 'Ã¼bers', '##etzung', 'Â«', 'La', 'gest', '##ion', 'de', 'l', \"'\", '[UNK]', 'se', 'fa', '##it', 'Ã ', 'di', '##ff', '##Ã©r', '##ents', 'ni', '##ve', '##aux', ',', 'de', 'l', \"'\", 'ad', '-', '[SEP]']\n",
      "Chunk 8: ['[CLS]', 'hin', 'zur', 'Lenk', '##ung', 'ganze', '##r', 'Bildungs', '##systeme', '(', 'Open', '##A', '##I', ',', '202', '4', ')', '.', 'Maschine', '##lle', 'Sprach', '-', 'Ã¼bers', '##etzung', 'Â«', 'La', 'gest', '##ion', 'de', 'l', \"'\", '[UNK]', 'se', 'fa', '##it', 'Ã ', 'di', '##ff', '##Ã©r', '##ents', 'ni', '##ve', '##aux', ',', 'de', 'l', \"'\", 'ad', '-', 'min', '##istr', '##ation', 'd', \"'\", '[UNK]', 'd', \"'\", 'en', '##sei', '##gne', '##ment', 'individ', '##uel', '##s', 'tel', '##s', 'qu', '##e', 'les', '[UNK]', ',', 'les', 'univers', '##itÃ©', '##s', 'o', '##u', 'les', 'ce', '##nt', '##res', 'de', 'form', '##ation', 'con', '-', 'ti', '##nue', 'Ã ', 'la', 'gest', '##ion', 'de', 'sy', '##st', '##Ã¨', '##mes', '[UNK]', 'comp', '##let', '##s', 'Â»', '(', 'De', '##ep', '##L', ',', '202', '##4', ')', '.', 'Bild', 'inkl', '.', 'Prom', '##pt', 'Â«', 'Symbol', '##ik', 'fÃ¼r', 'Bildungs', '##management', 'Â»', '(', 'DA', '##LL', '-', '[SEP]']\n",
      "Chunk 9: ['[CLS]', ':', 'z', '.', 'B', '.', 'Open', '##A', '##I', 'als', 'Autor', 'des', 'Lar', '##ge', 'Lang', '##uage', 'Model', '##s', 'Chat', '##G', '##PT', 'âˆ’', 'Datum', ':', 'das', 'Jahr', 'in', 'welchem', 'ein', 'K', '##I', '-', 'Tool', 'genutzt', 'wurde', 'âˆ’', 'Bei', 'Bildern', 'Bild', '##bezeichnung', 'mit', 'Initi', '##alp', '##rom', '##pt', ':', 'in', 'Anf', '##Ã¼hrung', '##s', '-', 'und', 'Schluss', '##zei', '-', 'che', '##n', 'Zit', '##ier', '##vor', '##gaben', 'Z', '-', 'MB', '-', 'Merk', '##blatt', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Version', ':', '1', '.', '2', '.', '0', 'gÃ¼ltig', 'ab', ':', '01', '.', '03', '.', '202', '##5', 'Seite', '4', 'von', '5', 'Rektor', '##at', 'Ressort', 'Bildung', '5', '.', 'Verzeichnis', '-', 'Referenz', ':', 'Hilfsmittel', '##verzeichnis', 'inkl', '.', 'Verwendungs', '##zweck', 'Sie', 'mÃ¼ssen', 'v', 'erwe', '##nde', '##te', 'gener', '##ative', 'K', '##I', '-', 'Tool', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Sahajtomar/German-semantic\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings for Each Chunk\n",
    "\n",
    "Now we convert each text chunk into a numerical embedding that captures its semantic meaning. These embeddings will be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68843/4090508586.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim AusfÃ¼hren von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestÃ¼rzt. \n",
      "\u001b[1;31mBitte Ã¼berprÃ¼fen Sie den Code in der/den Zelle(n), um eine mÃ¶gliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "chunk_embeddings = model.encode(token_split_texts, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a FAISS Vector Store\n",
    "\n",
    "FAISS is a powerful library for efficient similarity search. Here, we build an index from our embeddings. Remember, FAISS only stores the numerical vectors so we must keep our original text mapping separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('faiss'):\n",
    "    os.makedirs('faiss')\n",
    "    \n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "print(len(token_split_texts_2))\n",
    "print(len(token_split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Projecting Embeddings with UMAP\n",
    "\n",
    "To visualize high-dimensional embeddings, we use UMAP to project them into 2D space. You can project both the entire dataset and individual query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit UMAP on the full dataset embeddings\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(chunk_embeddings)\n",
    "\n",
    "def project_embeddings(embeddings, umap_transform):\n",
    "    \"\"\"\n",
    "    Project a set of embeddings using a pre-fitted UMAP transform.\n",
    "    \"\"\"\n",
    "    umap_embeddings = np.empty((len(embeddings), 2))\n",
    "    for i, embedding in enumerate(tqdm.tqdm(embeddings, desc=\"Projecting Embeddings\")):\n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the entire dataset embeddings\n",
    "projected_dataset_embeddings = project_embeddings(chunk_embeddings, umap_transform)\n",
    "print(\"Projected dataset embeddings shape:\", projected_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Vector Store and Projecting Results\n",
    "\n",
    "We now define a retrieval function that takes a text query, embeds it, and searches our FAISS index for similar documents. We then project these result embeddings with UMAP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])\n",
    "    return retrieved_texts, retrieved_embeddings, distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"KI wÃ¤hrend der Bachelorarbeit\"\n",
    "results, result_embeddings, distances = retrieve(query, k=3)\n",
    "print(\"Retrieved document preview:\")\n",
    "print(results[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the result embeddings\n",
    "projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n",
    "\n",
    "# Also embed and project the original query for visualization\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "project_original_query = project_embeddings(query_embedding, umap_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Results\n",
    "\n",
    "Finally, we create a scatter plot to visualize the entire dataset, the retrieved results, and the original query in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shorten_text(text, max_length=15):\n",
    "    \"\"\"Shortens text to max_length and adds an ellipsis if shortened.\"\"\"\n",
    "    return (text[:max_length] + '...') if len(text) > max_length else text\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Scatter plots\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1],\n",
    "            s=10, color='gray', label='Dataset')\n",
    "plt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='g', label='Results')\n",
    "plt.scatter(project_original_query[:, 0], project_original_query[:, 1],\n",
    "            s=150, marker='X', color='r', label='Original Query')\n",
    "\n",
    "# If results is a list of texts, iterate directly\n",
    "for i, text in enumerate(results):\n",
    "    if i < len(projected_result_embeddings):\n",
    "        plt.annotate(shorten_text(text),\n",
    "                     (projected_result_embeddings[i, 0], projected_result_embeddings[i, 1]),\n",
    "                     fontsize=8)\n",
    "\n",
    "# Annotate the original query point\n",
    "original_query_text = 'Welche hilfsmittel sind erlaubt?'  # Replace with your actual query text if needed\n",
    "original_query_text = 'Wieviele Seiten muss die Arbeit sein?'  # Replace with your actual query text if needed\n",
    "\n",
    "plt.annotate(shorten_text(original_query_text),\n",
    "             (project_original_query[0, 0], project_original_query[0, 1]),\n",
    "             fontsize=8)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ Task: Semantic Retrieval-Augmented Question Answering Using Groq LLM\n",
    "\n",
    "## Objective\n",
    "Implement a question-answering system that:\n",
    "1. Retrieves the most semantically relevant text passages to a user query.\n",
    "2. Constructs a natural language prompt based on the retrieved content.\n",
    "3. Uses a large language model (LLM) hosted by Groq to generate an answer.\n",
    "\n",
    "---\n",
    "\n",
    "## Task Breakdown\n",
    "\n",
    "### 1. Embedding-Based Semantic Retrieval\n",
    "- Use the `SentenceTransformer` model `\"Sahajtomar/German-semantic\"` to encode a user query into a dense vector embedding.\n",
    "- Perform a nearest-neighbor search in a prebuilt FAISS index to retrieve the top-**k** similar text chunks. You can **use the prebuilt FAISS form above**.\n",
    "\n",
    "\n",
    "### 2. LLM Prompt Construction and Query Answering\n",
    "- Build the prompt:\n",
    "  - Using the retrieved text chunks, concatenates the results into a context block.\n",
    "  - Builds a **prompt** asking the LLM to answer the question using that context.\n",
    "  - Sends the prompt to the **Groq LLM API** (`llama-3.3-70b-versatile`) and returns the response.\n",
    "\n",
    "### 3. User Query Execution\n",
    "- An example query (`\"What is the most important factor in diagnosing asthma?\"`) is used to demonstrate the pipeline.\n",
    "- The final answer from the LLM is printed.\n",
    "\n",
    "\n",
    "## Tools & Models Used\n",
    "- **SentenceTransformers** (`Sahajtomar/German-semantic`) for embedding generation.\n",
    "- **FAISS** for efficient vector similarity search.\n",
    "- **Groq LLM API** (`llama-3.3-70b-versatile`) for generating the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ API Key: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Lade .env aus dem env-Unterordner\n",
    "load_dotenv(dotenv_path=\"env/.env\")\n",
    "\n",
    "# Zugriff auf die Variable\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "print(\"GROQ API Key:\", groq_api_key)  # Testausgabe (nur zum Testen!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort der API: 200\n",
      "{'id': 'chatcmpl-46d45619-d366-48af-8f50-1b90dd08ab07', 'object': 'chat.completion', 'created': 1747872066, 'model': 'llama3-70b-8192', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Der wichtigste Faktor bei der Diagnose von Asthma ist die Anamnese, also die Erfassung der Krankheitsgeschichte und der Symptome durch Befragung des Patienten.'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'queue_time': 0.09352358699999999, 'prompt_tokens': 88, 'prompt_time': 0.002425165, 'completion_tokens': 46, 'completion_time': 0.131428571, 'total_tokens': 134, 'total_time': 0.133853736}, 'usage_breakdown': {'models': None}, 'system_fingerprint': 'fp_dd4ae1c591', 'x_groq': {'id': 'req_01jvtmpacre0d8kaq9k32cxk9q'}}\n",
      "Antwort des LLM:\n",
      "Der wichtigste Faktor bei der Diagnose von Asthma ist die Anamnese, also die Erfassung der Krankheitsgeschichte und der Symptome durch Befragung des Patienten.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 1. Frage formulieren\n",
    "query = \"Was ist der wichtigste Faktor bei der Diagnose von Asthma?\"\n",
    "\n",
    "# 2. LLM-Embedding-Modell laden\n",
    "model = SentenceTransformer(\"Sahajtomar/German-semantic\")\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "# 3. FAISS-Index und zugehÃ¶rige Chunks laden\n",
    "index = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "\n",
    "# 4. Top-k Ã¤hnliche Chunks abrufen\n",
    "k = 5\n",
    "D, I = index.search(query_embedding, k)\n",
    "\n",
    "# Schutz gegen ungÃ¼ltige Indizes\n",
    "retrieved_chunks = [token_split_texts_2[i] for i in I[0] if i < len(token_split_texts_2)]\n",
    "\n",
    "# 5. Prompt konstruieren\n",
    "context = \"\\n\\n\".join(retrieved_chunks)\n",
    "prompt = f\"\"\"\n",
    "Du bist ein medizinischer Assistent. Beantworte die folgende Frage **nur** basierend auf dem gegebenen Kontext.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Frage: {query}\n",
    "Antwort:\n",
    "\"\"\"\n",
    "\n",
    "# 6. Anfrage an das Groq-LLM\n",
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "payload = {\n",
    "    \"model\": \"llama3-70b-8192\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher medizinischer Assistent auf Deutsch.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "# Troubleshooting\n",
    "print(\"Antwort der API:\", response.status_code)\n",
    "print(response.json())\n",
    "\n",
    "answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# 7. Ausgabe\n",
    "print(\"Antwort des LLM:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
