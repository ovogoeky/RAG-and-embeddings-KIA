{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a FAISS-Based Vector Store: A Journey Through Data Processing and Visualization\n",
    "\n",
    "In this notebook, you'll learn how to transform raw PDF documents into a searchable vector store using FAISS. We'll go on a journey where we:\n",
    "\n",
    "1. **Read and extract text from PDF files.**\n",
    "2. **Split the text into manageable chunks.**\n",
    "3. **Display tokenization outputs from different tokenizers.**\n",
    "4. **Generate embeddings from the text using a SentenceTransformer.**\n",
    "5. **Store the embeddings in a FAISS index.**\n",
    "6. **Project the embeddings into 2D space using UMAP for visualization.**\n",
    "7. **Visualize the entire process on a scatter plot.**\n",
    "8. **Incect your data into a prompt for a large language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data from PDFs\n",
    "\n",
    "First, we load PDF files from a directory, extract their text content, and combine it into one large text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Z_MB_Merkblatt_Verwendung_von_generativer_KI_in_Arbeiten.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:00<00:04,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Z_RL_Richtlinie_KI_bei_Leistungsnachweisen.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:00<00:03,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Bibliotheksangebot_Bachelorarbeit_HS24FS25.pdf'>\n",
      "<_io.BufferedReader name='data/ZHAW_Zitierleitfaden_DE.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:01<00:02,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/Z_RL_Richtlinie_Anhang_Deklarationspflicht_KI_bei_Arbeiten.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:02<00:01,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/05_Checkliste_Sprachliche_Formale_Ausarbeitung.pdf'>\n",
      "<_io.BufferedReader name='data/Schwerpunktthemen_fuer_Studenten.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:02<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedReader name='data/02_Merkblatt_Vermeidung-von-Plagiaten_0916.pdf'>\n",
      "<_io.BufferedReader name='data/W_MB_Merkblatt_Bachelorarbeit_BSc.pdf'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:03<00:00,  2.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Z-MB-Merkblatt Verwendung von  \\ngenerativer KI bei'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        print(file)\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting the Text into Chunks\n",
    "\n",
    "Large texts can be difficult to work with. We use a text splitter, in this case [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/),  to break the full text into smaller, overlapping chunks. This helps preserve context when we later embed the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter: 2000 characters per chunk with an overlap of 200 characters\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 62\n",
      "Preview of the first chunk: Z-MB-Merkblatt Verwendung von  \n",
      "generativer KI bei Arbeiten  \n",
      "Version:  1.2.0 gültig ab:  01.03.2025   Seite 1 von 5 \n",
      " Rektorat  \n",
      "Ressort Bildung  \n",
      "Verwendung von generativer KI bei Arbeiten  \n",
      "Dieses \n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Text with Different Tokenizers\n",
    "\n",
    "Before embedding, it's insightful to see how different tokenizers break up our text. Here, we use the tokenizer from the SentenceTransformer model (see [SentenceTransformersTokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html#sentencetransformerstokentextsplitter))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 254\n",
      "Z-MB-Merkblatt Verwendung von generativer KI bei Arbeiten Version: 1.2.0 gültig ab: 01.03.2025 Seite 1 von 5 Rektorat Ressort Bildung Verwendung von generativer KI bei Arbeiten Dieses Merkblatt basiert auf : − Richtlinie KI bei Leistungsnachweisen − ergänzender Anhang 1. Grundsatz Grundsätzlich gilt, dass die Verwendung von generativen KI -Tools bei Arbeiten zu de- klarieren ist. Dieses Merkblatt hilft Ihnen Schritt -für-Schritt bei der korrekten Umset- zung. 2. Pflichten im Umgang Ihre Pflichten im Umgang mit generativer KI − Sie\n"
     ]
    }
   ],
   "source": [
    "token_split_texts = []\n",
    "for text in chunks:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")\n",
    "print(token_split_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['<s>', '▁Z', '-', 'MB', '-', 'Mer', 'k', 'blatt', '▁Verwendung', '▁von', '▁genera', 'ti', 'ver', '▁KI', '▁bei', '▁Arbeiten', '▁Version', ':', '▁1.', '2.0', '▁gültig', '▁ab', ':', '▁01.', '03.20', '25', '▁Seite', '▁1', '▁von', '▁5', '▁Rektor', 'at', '▁Res', 'sort', '▁Bildung', '▁Verwendung', '▁von', '▁genera', 'ti', 'ver', '▁KI', '▁bei', '▁Arbeiten', '▁Dieses', '▁Merk', 'blatt', '▁bas', 'iert', '▁auf', '▁:', '▁−', '▁', 'Richtlinie', '▁KI', '▁bei', '▁Leistungs', 'nach', 'weise', 'n', '▁−', '▁ergänzen', 'der', '▁An', 'hang', '▁1.', '▁Grund', 'satz', '▁Grundsätzlich', '▁gilt', ',', '▁dass', '▁die', '▁Verwendung', '▁von', '▁genera', 'tive', 'n', '▁KI', '▁-', 'Tool', 's', '▁bei', '▁Arbeiten', '▁zu', '▁de', '-', '▁klar', 'ieren', '▁ist', '.', '▁Dieses', '▁Merk', 'blatt', '▁hilft', '▁Ihnen', '▁Schritt', '▁-', 'für', '-', 'Sch', 'rit', 't', '▁bei', '▁der', '▁korrekt', 'en', '▁Um', 'set', '-', '▁', 'zung', '.', '▁2.', '▁Pflicht', 'en', '▁im', '▁Umgang', '▁Ihre', '▁Pflicht', 'en', '▁im', '▁Umgang', '▁mit', '▁genera', 'ti', 'ver', '▁KI', '</s>']\n",
      "Chunk 1: ['<s>', '▁tragen', '▁Verantwortung', '▁für', '▁die', '▁Qualität', '▁der', '▁Inhalte', '▁:', '▁Auch', '▁bei', '▁korrekt', 'er', '▁Deklar', 'ation', '▁bleiben', '▁Sie', '▁verpflichtet', ',', '▁die', '▁Re', 'leva', 'nz', ',', '▁den', '▁Wahrheit', 's', 'ge', 'halt', '▁oder', '▁die', '▁Genau', 'igkeit', '▁sowohl', '▁der', '▁eingesetzt', 'en', '▁genera', 'tive', 'n', '▁KI', '▁-', 'Tool', 's', '▁als', '▁auch', '▁des', '▁Out', 'put', 's', '▁zu', '▁überprüfen', '.', '▁−', '▁Die', '▁Arbeit', '▁muss', '▁Ihre', '▁eigen', 'ständig', 'e', '▁Leistung', '▁sein', '▁:', '▁Genera', 'tive', '▁KI', '▁darf', '▁aus', 'sch', 'lies', 's', 'lich', '▁unterstützen', 'd', '▁eingesetzt', '▁werden', '.', '▁Der', '▁Anteil', '▁bzw', '.', '▁das', '▁Aus', 'mas', 's', '▁des', '▁Mit', 'wirken', 's', '▁von', '▁genera', 'tive', 'n', '▁KI', '▁-', 'Tool', 's', '▁an', '▁der', '▁sch', 'öpfe', 'r', 'ischen', '▁Leistung', '▁Ihrer', '▁Arbeit', '▁muss', '▁für', '▁Dritte', '▁erken', 'n', 'bar', '▁sein', '.', '▁−', '▁Sie', '▁dürfen', '▁nicht', '▁jede', '▁Art', '▁von', '▁Daten', '▁mit', '▁genera', 'tive', 'n', '</s>']\n",
      "Chunk 2: ['<s>', '▁System', 'en', '▁be', 'arbeiten', '▁:', '▁Grundsätzlich', '▁ist', '▁die', '▁Bearbeitung', '▁von', '▁folgenden', '▁Art', '▁Daten', '▁nicht', '▁erlaubt', ':', '▁besonders', '▁sch', 'üt', '-', '▁zen', 's', 'werte', '▁Per', 's', '▁on', 'en', 'daten', '▁(', 'z', '.', 'B', '.', '▁Daten', '▁über', '▁religi', 'ös', 'e', '▁Ansicht', 'en', ',', '▁Gesundheit', ',', '▁oder', '▁gene', 'ti', '-', '▁', 'sche', '▁Daten', ');', '▁Betriebs', '▁-', '▁und', '▁Geschäfts', 'ge', 'heim', 'nisse', '▁(', 'jur', 'istische', 'r', '▁Personen', ').', '▁Z', '-', 'MB', '-', 'Mer', 'k', 'blatt', '▁Verwendung', '▁von', '▁genera', 'ti', 'ver', '▁KI', '▁bei', '▁Arbeiten', '▁Version', ':', '▁1.', '2.0', '▁gültig', '▁ab', ':', '▁01.', '03.20', '25', '▁Seite', '▁2', '▁von', '▁5', '▁Rektor', 'at', '▁Res', 'sort', '▁Bildung', '▁3.', '▁Übersicht', ':', '▁Schritt', '▁für', '▁Schritt', '▁zur', '▁korrekt', 'en', '▁Deklar', 'ation', '▁Diese', '▁Angaben', '▁dienen', '▁der', '▁Orient', 'ierung', '.', '▁Im', '▁Zweifel', 's', 'fall', '▁konsult', 'ieren', '▁Sie', '▁bitte', '▁den', '▁Rich', '</s>']\n",
      "Chunk 3: ['<s>', '▁nie', 'n', '-', 'An', 'hang', '▁Deklar', 'a', '▁', 'tions', 'pflicht', '▁von', '▁genera', 'ti', 'ver', '▁KI', '▁bei', '▁Arbeiten', '▁oder', '▁kontaktieren', '▁Ihre', '▁Do', '-', '▁zen', 'tin', '▁oder', '▁Ihren', '▁Do', 'zen', 'ten', '▁', '.', '▁Hat', '▁Ihr', '▁Studien', 'gang', '▁oder', '▁Do', 'zen', 't', ':', 'in', '▁für', '▁Ihre', '▁Arbeit', '▁Regeln', '▁', 'bezüglich', '▁Einsatz', '▁genera', 'ti', 'ver', '▁KI', '▁-', 'Tool', 's', '▁festgelegt', '?', '▁Ja', '▁Es', '▁gelten', '▁die', '</s>']\n",
      "Chunk 4: ['<s>', '▁zen', 'tin', '▁oder', '▁Ihren', '▁Do', 'zen', 'ten', '▁', '.', '▁Hat', '▁Ihr', '▁Studien', 'gang', '▁oder', '▁Do', 'zen', 't', ':', 'in', '▁für', '▁Ihre', '▁Arbeit', '▁Regeln', '▁', 'bezüglich', '▁Einsatz', '▁genera', 'ti', 'ver', '▁KI', '▁-', 'Tool', 's', '▁festgelegt', '?', '▁Ja', '▁Es', '▁gelten', '▁die', '▁An', 'weis', 'ungen', '▁Ihrer', '▁Do', 'zen', 'tin', '▁oder', '▁Ihres', '▁Do', 'zen', 'ten', '.', 'We', 'is', 's', '▁nicht', '▁Inform', 'ieren', '▁Sie', '▁sich', '▁bei', '▁Ihrer', '▁Do', 'zen', 'tin', '▁oder', '▁Ihrem', '▁Do', 'zen', 'ten', '.', 'Ne', 'in', '▁Haben', '▁Sie', '▁bei', '▁der', '▁Er', 'stellung', '▁Ihrer', '▁Arbeit', '▁genera', 'tive', '▁KI', '▁-', 'Tool', 's', '▁verwendet', '?', '▁Nein', '▁—', '▁—', '▁—', 'Ja', ',', '▁-', 'als', '▁Formul', 'ierung', 's', 'hilfe', '▁-', 'als', '▁Rechts', 'schreib', 'e', 'hilfe', '▁-', 'zur', '▁Sprache', 'rg', 'än', 'zung', '▁—', '▁—', '▁—', '-', 'als', '▁Gedanken', 'an', 'stos', 's', '▁-', 'als', '▁Inspiration', 'sque', '</s>']\n",
      "Chunk 5: ['<s>', '▁—', '▁✓', '-', 'Out', 'put', '▁', 'wort', 'w', 'ört', 'lich', '▁oder', '▁sinn', 'ge', 'mä', 's', 's', '▁-', 'zur', '▁Bilder', 'stellung', '▁✓', '▁✓', '▁✓', 'ander', 'weit', 'ig', '▁zur', '▁-', 'Text', 'er', 'stellung', '▁-', '(', 'We', 'iter', '▁-', ')', 'Be', 'arbeit', 'ung', '▁—', '▁✓', '▁✓', '▁VER', 'ZE', 'ICH', 'NIS', '▁-', 'RE', 'F', 'EREN', 'Z', '▁KI', '-', 'Tool', 's', '▁und', '▁Verwendung', 's', 'zweck', '▁müssen', '▁auf', 'geführt', '▁werden', '▁', 'MET', 'HO', 'DEN', '▁-', 'RE', 'F', 'EREN', 'Z', '▁KI', '-', 'Nu', 't', 'zung', '▁muss', '▁beschrieben', '▁und', '▁reflekt', 'iert', '▁werden', '▁IN', '-', 'TEX', 'T', '▁-', 'RE', 'F', 'EREN', 'Z', '▁KI', '-', 'Out', 'put', '▁muss', '▁i', '▁m', '▁Text', '▁korrekt', '▁zit', 'iert', '▁werden', '▁Deklar', 'a', '▁tion', '▁als', '▁...', '▁Z', '-', 'MB', '-', 'Mer', 'k', 'blatt', '▁Verwendung', '▁von', '▁genera', 'ti', 'ver', '▁KI', '▁bei', '▁Arbeiten', '▁Version', '</s>']\n",
      "Chunk 6: ['<s>', '▁2.0', '▁gültig', '▁ab', ':', '▁01.', '03.20', '25', '▁Seite', '▁3', '▁von', '▁5', '▁Rektor', 'at', '▁Res', 'sort', '▁Bildung', '▁4.', '▁In', '-', 'Text', '-', 'Ref', 'eren', 'z', ':', '▁Out', 'put', '▁im', '▁Text', '▁zit', 'ieren', '▁Sie', '▁müssen', '▁Out', 'put', '▁von', '▁genera', 'tive', 'n', '▁KI', '▁-', 'Tool', 's', '▁', ',', '▁welchen', '▁Sie', '▁', 'wort', 'w', 'ört', 'lich', '▁oder', '▁para', 'phra', '-', '▁si', 'erend', '▁in', '▁Ihrer', '▁Arbeit', '▁verwenden', '▁', ',', '▁zit', 'ieren', '.', '▁Dies', '▁gilt', '▁auch', '▁für', '▁den', '▁Fall', ',', '▁dass', '▁Sie', '▁diesen', '▁nur', '▁auszu', 'g', 'sweise', '▁oder', '▁sinn', 'ge', 'mä', 's', 's', '▁übernehmen', '.', '▁4.1', '▁Zi', 'tier', 'vor', 'gabe', '▁4.2', '▁Beispiel', 'e', '▁von', '▁Kurz', 'v', 'erweise', 'n', '▁im', '▁Text', '▁Art', '▁der', '▁Nutzung', '▁Beispiel', '▁W', 'ört', 'liche', '▁Über', 'nahme', '▁«', 'Bild', 'ungs', 'management', '▁findet', '▁auf', '▁verschiedenen', '▁Ebene', 'n', '▁statt', ',', '▁von', '▁der', '</s>']\n",
      "Chunk 7: ['<s>', '▁r', '▁Bildungs', 'ein', 'richtung', 'en', '▁wie', '▁Schu', '-', '▁len', ',', '▁Hochschule', 'n', '▁oder', '▁Weiterbildung', 'szent', 'ren', '▁bis', '▁hin', '▁zur', '▁Ste', 'u', '-', '▁', 'erung', '▁ganz', 'er', '▁Bildungs', 'system', 'e', '▁»', '▁(', 'Open', 'AI', ',', '▁202', '▁4)', '.', '▁Para', 'phra', 's', 'ierung', '▁Bildungs', 'management', '▁erfolgt', '▁auf', '▁unterschiedlich', 'en', '▁Ebene', 'n', ',', '▁von', '▁der', '▁Ad', '▁ministra', 'tion', '▁individuelle', 'r', '▁Bildungs', 'institution', 'en', '▁bis', '▁hin', '▁zur', '▁Len', 'kung', '▁ganz', 'er', '▁Bildungs', 'system', 'e', '▁(', 'Open', 'AI', ',', '▁202', '▁4)', '.', '▁Mas', 'chi', 'nelle', '▁Sprach', '-', '▁über', 'setzung', '▁«', 'La', '▁gestion', '▁de', '▁l', \"'\", 'éducation', '▁se', '▁fait', '▁à', '▁différents', '▁niveaux', ',', '▁de', '▁l', \"'\", 'ad', '-', '</s>']\n",
      "Chunk 8: ['<s>', '▁hin', '▁zur', '▁Len', 'kung', '▁ganz', 'er', '▁Bildungs', 'system', 'e', '▁(', 'Open', 'AI', ',', '▁202', '▁4)', '.', '▁Mas', 'chi', 'nelle', '▁Sprach', '-', '▁über', 'setzung', '▁«', 'La', '▁gestion', '▁de', '▁l', \"'\", 'éducation', '▁se', '▁fait', '▁à', '▁différents', '▁niveaux', ',', '▁de', '▁l', \"'\", 'ad', '-', '▁ministra', 'tion', '▁d', \"'\", 'établissement', 's', '▁d', \"'\", 'enseignement', '▁individuel', 's', '▁tel', 's', '▁que', '▁les', '▁écoles', ',', '▁les', '▁', 'université', 's', '▁ou', '▁les', '▁centres', '▁de', '▁formation', '▁con', '-', '▁tin', 'ue', '▁à', '▁la', '▁gestion', '▁de', '▁système', 's', '▁é', 'duc', 'atif', 's', '▁complet', 's', '▁»', '▁(', 'De', 'ep', 'L', ',', '▁20', '24)', '.', '▁Bild', '▁inkl', '.', '▁Pro', 'mp', 't', '▁«', 'Sy', 'mbo', 'lik', '▁für', '▁Bildungs', 'management', '▁»', '▁(', 'D', 'ALL', '▁-', 'E', '▁2', ',', '▁Open', 'AI', ',', '▁202', '▁4)', '▁Zu', 'rück', '▁zur', '▁Übersicht', '▁↑', '▁−', '▁Autor', '▁des', '</s>']\n",
      "Chunk 9: ['<s>', '▁:', '▁z', '.', 'B', '.', '▁Open', 'AI', '▁als', '▁Autor', '▁des', '▁Large', '▁Language', '▁Model', 's', '▁Chat', 'G', 'PT', '▁−', '▁Datum', '▁:', '▁das', '▁Jahr', '▁in', '▁welche', 'm', '▁ein', '▁KI', '▁-', 'Tool', '▁genutzt', '▁wurde', '▁−', '▁Bei', '▁Bilder', 'n', '▁Bild', 'be', 'zeichnung', '▁mit', '▁In', 'iti', 'al', 'pro', 'mp', 't', '▁:', '▁in', '▁An', 'führung', 's', '▁-', '▁und', '▁Schluss', 'ze', 'i', '-', '▁che', 'n', '▁Zi', 'tier', 'vor', 'gaben', '▁Z', '-', 'MB', '-', 'Mer', 'k', 'blatt', '▁Verwendung', '▁von', '▁genera', 'ti', 'ver', '▁KI', '▁bei', '▁Arbeiten', '▁Version', ':', '▁1.', '2.0', '▁gültig', '▁ab', ':', '▁01.', '03.20', '25', '▁Seite', '▁4', '▁von', '▁5', '▁Rektor', 'at', '▁Res', 'sort', '▁Bildung', '▁5.', '▁Ver', 'ze', 'ich', 'nis', '▁-', 'Ref', 'eren', 'z', ':', '▁Hilfs', 'mittel', 'ver', 'ze', 'ich', 'nis', '▁inkl', '.', '▁Verwendung', 's', 'zweck', '▁Sie', '▁müssen', '▁v', '▁er', 'wende', 'te', '▁genera', 'tive', '▁KI', '</s>']\n"
     ]
    }
   ],
   "source": [
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['[CLS]', 'Z', '-', 'MB', '-', 'Merk', '##blatt', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Version', ':', '1', '.', '2', '.', '0', 'gültig', 'ab', ':', '01', '.', '03', '.', '202', '##5', 'Seite', '1', 'von', '5', 'Rektor', '##at', 'Ressort', 'Bildung', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Dieses', 'Merk', '##blatt', 'basiert', 'auf', ':', '−', 'Richtlinie', 'K', '##I', 'bei', 'Leistungs', '##nach', '##weisen', '−', 'ergänzen', '##der', 'Anhang', '1', '.', 'Grundsatz', 'Grundsätzlich', 'gilt', ',', 'dass', 'die', 'Verwendung', 'von', 'gener', '##ativen', 'K', '##I', '-', 'Tool', '##s', 'bei', 'Arbeiten', 'zu', 'de', '-', 'klar', '##ieren', 'ist', '.', 'Dieses', 'Merk', '##blatt', 'hilft', 'Ihnen', 'Schritt', '-', 'für', '-', 'Schritt', 'bei', 'der', 'korrekt', '##en', 'Um', '##set', '-', 'zun', '##g', '.', '2', '.', 'Pflichten', 'im', 'Umgang', 'Ihre', 'Pflichten', 'im', 'Umgang', 'mit', 'gener', '##ativer', 'K', '##I', '−', 'Sie', '[SEP]']\n",
      "Chunk 1: ['[CLS]', 'tragen', 'Verantwortung', 'für', 'die', 'Qualität', 'der', 'Inhalte', ':', 'Auch', 'bei', 'korrekt', '##er', 'Dek', '##lar', '##ation', 'bleiben', 'Sie', 'verpflichtet', ',', 'die', 'Relevanz', ',', 'den', 'Wahrheit', '##sg', '##eha', '##lt', 'oder', 'die', 'Genauigkeit', 'sowohl', 'der', 'eingesetzten', 'gener', '##ativen', 'K', '##I', '-', 'Tool', '##s', 'als', 'auch', 'des', 'Out', '##put', '##s', 'zu', 'überprüfen', '.', '−', 'Die', 'Arbeit', 'muss', 'Ihre', 'eigenständige', 'Leistung', 'sein', ':', 'Gener', '##ative', 'K', '##I', 'darf', 'aussch', '##liesslich', 'unterstützen', '##d', 'eingesetzt', 'werden', '.', 'Der', 'Anteil', 'bzw', '.', 'das', 'Aus', '##mass', 'des', 'Mit', '##wirken', '##s', 'von', 'gener', '##ativen', 'K', '##I', '-', 'Tool', '##s', 'an', 'der', 'sch', '##öpfe', '##rischen', 'Leistung', 'Ihrer', 'Arbeit', 'muss', 'für', 'Dritte', 'erkennbar', 'sein', '.', '−', 'Sie', 'dürfen', 'nicht', 'jede', 'Art', 'von', 'Daten', 'mit', 'gener', '##ativen', 'K', '##I', '-', '[SEP]']\n",
      "Chunk 2: ['[CLS]', 'Systemen', 'bearbeiten', ':', 'Grundsätzlich', 'ist', 'die', 'Bearbeitung', 'von', 'folgenden', 'Art', 'Daten', 'nicht', 'erlaubt', ':', 'besonders', 'sch', '##üt', '-', 'z', '##ens', '##werte', 'Pers', 'one', '##nd', '##aten', '(', 'z', '.', 'B', '.', 'Daten', 'über', 'religiöse', 'Ansichten', ',', 'Gesundheit', ',', 'oder', 'gen', '##eti', '-', 'sch', '##e', 'Daten', ')', ';', 'Betriebs', '-', 'und', 'Geschäfts', '##geheim', '##nisse', '(', 'juristische', '##r', 'Personen', ')', '.', 'Z', '-', 'MB', '-', 'Merk', '##blatt', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Version', ':', '1', '.', '2', '.', '0', 'gültig', 'ab', ':', '01', '.', '03', '.', '202', '##5', 'Seite', '2', 'von', '5', 'Rektor', '##at', 'Ressort', 'Bildung', '3', '.', 'Übersicht', ':', 'Schritt', 'für', 'Schritt', 'zur', 'korrekt', '##en', 'Dek', '##lar', '##ation', 'Diese', 'Angaben', 'dienen', 'der', 'Orientierung', '.', 'Im', 'Zweifel', '##sf', '##all', 'konsul', '##tieren', 'Sie', 'bitte', 'den', 'Richt', '##li', '[SEP]']\n",
      "Chunk 3: ['[CLS]', 'nie', '##n', '-', 'Anhang', 'Dek', '##lar', '##a', 'ti', '##ons', '##pflicht', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'oder', 'kontaktieren', 'Ihre', 'Do', '-', 'z', '##entin', 'oder', 'Ihren', 'Dozent', '##en', '.', 'Hat', 'Ihr', 'Studien', '##gang', 'oder', 'Dozent', ':', 'in', 'für', 'Ihre', 'Arbeit', 'Regeln', 'bezüglich', 'Einsatz', 'gener', '##ativer', 'K', '##I', '-', 'Tool', '##s', 'festgelegt', '?', 'Ja', 'Es', 'gelten', 'die', '[SEP]']\n",
      "Chunk 4: ['[CLS]', 'z', '##entin', 'oder', 'Ihren', 'Dozent', '##en', '.', 'Hat', 'Ihr', 'Studien', '##gang', 'oder', 'Dozent', ':', 'in', 'für', 'Ihre', 'Arbeit', 'Regeln', 'bezüglich', 'Einsatz', 'gener', '##ativer', 'K', '##I', '-', 'Tool', '##s', 'festgelegt', '?', 'Ja', 'Es', 'gelten', 'die', 'Anweisungen', 'Ihrer', 'Dozent', '##in', 'oder', 'Ihres', 'Dozent', '##en', '.', 'Weiss', 'nicht', 'Inform', '##ieren', 'Sie', 'sich', 'bei', 'Ihrer', 'Dozent', '##in', 'oder', 'Ihrem', 'Dozent', '##en', '.', 'Nein', 'Haben', 'Sie', 'bei', 'der', 'Erstellung', 'Ihrer', 'Arbeit', 'gener', '##ative', 'K', '##I', '-', 'Tool', '##s', 'verwendet', '?', 'Nein', '—', '—', '—', 'Ja', ',', '-', 'als', 'Formulierung', '##shi', '##lf', '##e', '-', 'als', 'Rechts', '##sch', '##reibe', '##hilfe', '-', 'zur', 'Sprache', '##rg', '##än', '##zung', '—', '—', '—', '-', 'als', 'Gedanken', '##anst', '##oss', '-', 'als', 'Inspiration', '##s', '##quelle', '—', '[SEP]']\n",
      "Chunk 5: ['[CLS]', '—', '[UNK]', '-', 'Out', '##put', 'wor', '##tw', '##örtlich', 'oder', 'sinn', '##gemä', '##ss', '-', 'zur', 'Bilder', '##stellung', '[UNK]', '[UNK]', '[UNK]', 'zur', '-', 'Texte', '##rs', '##tell', '##ung', '-', '(', 'Weiter', '-', ')', 'Bearbeitung', '—', '[UNK]', '[UNK]', 'VER', '##ZE', '##ICH', '##NIS', '-', 'RE', '##FE', '##R', '##EN', '##Z', 'K', '##I', '-', 'Tool', '##s', 'und', 'Verwendungs', '##zweck', 'müssen', 'aufgeführt', 'werden', 'M', '##ET', '##HO', '##DEN', '-', 'RE', '##FE', '##R', '##EN', '##Z', 'K', '##I', '-', 'Nutzung', 'muss', 'beschrieben', 'und', 'refle', '##ktiert', 'werden', 'IN', '-', 'T', '##EX', '##T', '-', 'RE', '##FE', '##R', '##EN', '##Z', 'K', '##I', '-', 'Out', '##put', 'muss', 'i', 'm', 'Text', 'korrekt', 'zitiert', 'werden', 'Dek', '##lar', '##a', 'ti', '##on', 'als', '.', '.', '.', 'Z', '-', 'MB', '-', 'Merk', '##blatt', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Version', ':', '1', '.', '[SEP]']\n",
      "Chunk 6: ['[CLS]', '2', '.', '0', 'gültig', 'ab', ':', '01', '.', '03', '.', '202', '##5', 'Seite', '3', 'von', '5', 'Rektor', '##at', 'Ressort', 'Bildung', '4', '.', 'In', '-', 'Text', '-', 'Referenz', ':', 'Out', '##put', 'im', 'Text', 'zit', '##ieren', 'Sie', 'müssen', 'Out', '##put', 'von', 'gener', '##ativen', 'K', '##I', '-', 'Tool', '##s', ',', 'welchen', 'Sie', 'wor', '##tw', '##örtlich', 'oder', 'par', '##aph', '##ra', '-', 'sie', '##ren', '##d', 'in', 'Ihrer', 'Arbeit', 'verwenden', ',', 'zit', '##ieren', '.', 'Dies', 'gilt', 'auch', 'für', 'den', 'Fall', ',', 'dass', 'Sie', 'diesen', 'nur', 'auszu', '##gs', '##weise', 'oder', 'sinn', '##gemä', '##ss', 'übernehmen', '.', '4', '.', '1', 'Zit', '##ier', '##vor', '##gabe', '4', '.', '2', 'Beispiele', 'von', 'Kurz', '##verw', '##eisen', 'im', 'Text', 'Art', 'der', 'Nutzung', 'Beispiel', 'Wör', '##tliche', 'Übernahme', '«', 'Bildungs', '##management', 'findet', 'auf', 'verschiedenen', 'Ebenen', 'statt', ',', 'von', 'der', 'Verwaltung', 'einzelne', '[SEP]']\n",
      "Chunk 7: ['[CLS]', 'r', 'Bildungs', '##einrichtungen', 'wie', 'Schu', '-', 'len', ',', 'Hochschulen', 'oder', 'Weiterbildungs', '##zentren', 'bis', 'hin', 'zur', 'Ste', '##u', '-', 'er', '##ung', 'ganze', '##r', 'Bildungs', '##systeme', '»', '(', 'Open', '##A', '##I', ',', '202', '4', ')', '.', 'Par', '##aph', '##ras', '##ierung', 'Bildungs', '##management', 'erfolgt', 'auf', 'unterschiedlich', '##en', 'Ebenen', ',', 'von', 'der', 'Ad', 'min', '##istr', '##ation', 'individuelle', '##r', 'Bildungs', '##institutionen', 'bis', 'hin', 'zur', 'Lenk', '##ung', 'ganze', '##r', 'Bildungs', '##systeme', '(', 'Open', '##A', '##I', ',', '202', '4', ')', '.', 'Maschine', '##lle', 'Sprach', '-', 'übers', '##etzung', '«', 'La', 'gest', '##ion', 'de', 'l', \"'\", '[UNK]', 'se', 'fa', '##it', 'à', 'di', '##ff', '##ér', '##ents', 'ni', '##ve', '##aux', ',', 'de', 'l', \"'\", 'ad', '-', '[SEP]']\n",
      "Chunk 8: ['[CLS]', 'hin', 'zur', 'Lenk', '##ung', 'ganze', '##r', 'Bildungs', '##systeme', '(', 'Open', '##A', '##I', ',', '202', '4', ')', '.', 'Maschine', '##lle', 'Sprach', '-', 'übers', '##etzung', '«', 'La', 'gest', '##ion', 'de', 'l', \"'\", '[UNK]', 'se', 'fa', '##it', 'à', 'di', '##ff', '##ér', '##ents', 'ni', '##ve', '##aux', ',', 'de', 'l', \"'\", 'ad', '-', 'min', '##istr', '##ation', 'd', \"'\", '[UNK]', 'd', \"'\", 'en', '##sei', '##gne', '##ment', 'individ', '##uel', '##s', 'tel', '##s', 'qu', '##e', 'les', '[UNK]', ',', 'les', 'univers', '##ité', '##s', 'o', '##u', 'les', 'ce', '##nt', '##res', 'de', 'form', '##ation', 'con', '-', 'ti', '##nue', 'à', 'la', 'gest', '##ion', 'de', 'sy', '##st', '##è', '##mes', '[UNK]', 'comp', '##let', '##s', '»', '(', 'De', '##ep', '##L', ',', '202', '##4', ')', '.', 'Bild', 'inkl', '.', 'Prom', '##pt', '«', 'Symbol', '##ik', 'für', 'Bildungs', '##management', '»', '(', 'DA', '##LL', '-', '[SEP]']\n",
      "Chunk 9: ['[CLS]', ':', 'z', '.', 'B', '.', 'Open', '##A', '##I', 'als', 'Autor', 'des', 'Lar', '##ge', 'Lang', '##uage', 'Model', '##s', 'Chat', '##G', '##PT', '−', 'Datum', ':', 'das', 'Jahr', 'in', 'welchem', 'ein', 'K', '##I', '-', 'Tool', 'genutzt', 'wurde', '−', 'Bei', 'Bildern', 'Bild', '##bezeichnung', 'mit', 'Initi', '##alp', '##rom', '##pt', ':', 'in', 'Anf', '##ührung', '##s', '-', 'und', 'Schluss', '##zei', '-', 'che', '##n', 'Zit', '##ier', '##vor', '##gaben', 'Z', '-', 'MB', '-', 'Merk', '##blatt', 'Verwendung', 'von', 'gener', '##ativer', 'K', '##I', 'bei', 'Arbeiten', 'Version', ':', '1', '.', '2', '.', '0', 'gültig', 'ab', ':', '01', '.', '03', '.', '202', '##5', 'Seite', '4', 'von', '5', 'Rektor', '##at', 'Ressort', 'Bildung', '5', '.', 'Verzeichnis', '-', 'Referenz', ':', 'Hilfsmittel', '##verzeichnis', 'inkl', '.', 'Verwendungs', '##zweck', 'Sie', 'müssen', 'v', 'erwe', '##nde', '##te', 'gener', '##ative', 'K', '##I', '-', 'Tool', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Sahajtomar/German-semantic\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings for Each Chunk\n",
    "\n",
    "Now we convert each text chunk into a numerical embedding that captures its semantic meaning. These embeddings will be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68843/4090508586.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "chunk_embeddings = model.encode(token_split_texts, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a FAISS Vector Store\n",
    "\n",
    "FAISS is a powerful library for efficient similarity search. Here, we build an index from our embeddings. Remember, FAISS only stores the numerical vectors so we must keep our original text mapping separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('faiss'):\n",
    "    os.makedirs('faiss')\n",
    "    \n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "print(len(token_split_texts_2))\n",
    "print(len(token_split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Projecting Embeddings with UMAP\n",
    "\n",
    "To visualize high-dimensional embeddings, we use UMAP to project them into 2D space. You can project both the entire dataset and individual query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit UMAP on the full dataset embeddings\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(chunk_embeddings)\n",
    "\n",
    "def project_embeddings(embeddings, umap_transform):\n",
    "    \"\"\"\n",
    "    Project a set of embeddings using a pre-fitted UMAP transform.\n",
    "    \"\"\"\n",
    "    umap_embeddings = np.empty((len(embeddings), 2))\n",
    "    for i, embedding in enumerate(tqdm.tqdm(embeddings, desc=\"Projecting Embeddings\")):\n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the entire dataset embeddings\n",
    "projected_dataset_embeddings = project_embeddings(chunk_embeddings, umap_transform)\n",
    "print(\"Projected dataset embeddings shape:\", projected_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Vector Store and Projecting Results\n",
    "\n",
    "We now define a retrieval function that takes a text query, embeds it, and searches our FAISS index for similar documents. We then project these result embeddings with UMAP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])\n",
    "    return retrieved_texts, retrieved_embeddings, distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"KI während der Bachelorarbeit\"\n",
    "results, result_embeddings, distances = retrieve(query, k=3)\n",
    "print(\"Retrieved document preview:\")\n",
    "print(results[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the result embeddings\n",
    "projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n",
    "\n",
    "# Also embed and project the original query for visualization\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "project_original_query = project_embeddings(query_embedding, umap_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Results\n",
    "\n",
    "Finally, we create a scatter plot to visualize the entire dataset, the retrieved results, and the original query in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shorten_text(text, max_length=15):\n",
    "    \"\"\"Shortens text to max_length and adds an ellipsis if shortened.\"\"\"\n",
    "    return (text[:max_length] + '...') if len(text) > max_length else text\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Scatter plots\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1],\n",
    "            s=10, color='gray', label='Dataset')\n",
    "plt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='g', label='Results')\n",
    "plt.scatter(project_original_query[:, 0], project_original_query[:, 1],\n",
    "            s=150, marker='X', color='r', label='Original Query')\n",
    "\n",
    "# If results is a list of texts, iterate directly\n",
    "for i, text in enumerate(results):\n",
    "    if i < len(projected_result_embeddings):\n",
    "        plt.annotate(shorten_text(text),\n",
    "                     (projected_result_embeddings[i, 0], projected_result_embeddings[i, 1]),\n",
    "                     fontsize=8)\n",
    "\n",
    "# Annotate the original query point\n",
    "original_query_text = 'Welche hilfsmittel sind erlaubt?'  # Replace with your actual query text if needed\n",
    "original_query_text = 'Wieviele Seiten muss die Arbeit sein?'  # Replace with your actual query text if needed\n",
    "\n",
    "plt.annotate(shorten_text(original_query_text),\n",
    "             (project_original_query[0, 0], project_original_query[0, 1]),\n",
    "             fontsize=8)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📝 Task: Semantic Retrieval-Augmented Question Answering Using Groq LLM\n",
    "\n",
    "## Objective\n",
    "Implement a question-answering system that:\n",
    "1. Retrieves the most semantically relevant text passages to a user query.\n",
    "2. Constructs a natural language prompt based on the retrieved content.\n",
    "3. Uses a large language model (LLM) hosted by Groq to generate an answer.\n",
    "\n",
    "---\n",
    "\n",
    "## Task Breakdown\n",
    "\n",
    "### 1. Embedding-Based Semantic Retrieval\n",
    "- Use the `SentenceTransformer` model `\"Sahajtomar/German-semantic\"` to encode a user query into a dense vector embedding.\n",
    "- Perform a nearest-neighbor search in a prebuilt FAISS index to retrieve the top-**k** similar text chunks. You can **use the prebuilt FAISS form above**.\n",
    "\n",
    "\n",
    "### 2. LLM Prompt Construction and Query Answering\n",
    "- Build the prompt:\n",
    "  - Using the retrieved text chunks, concatenates the results into a context block.\n",
    "  - Builds a **prompt** asking the LLM to answer the question using that context.\n",
    "  - Sends the prompt to the **Groq LLM API** (`llama-3.3-70b-versatile`) and returns the response.\n",
    "\n",
    "### 3. User Query Execution\n",
    "- An example query (`\"What is the most important factor in diagnosing asthma?\"`) is used to demonstrate the pipeline.\n",
    "- The final answer from the LLM is printed.\n",
    "\n",
    "\n",
    "## Tools & Models Used\n",
    "- **SentenceTransformers** (`Sahajtomar/German-semantic`) for embedding generation.\n",
    "- **FAISS** for efficient vector similarity search.\n",
    "- **Groq LLM API** (`llama-3.3-70b-versatile`) for generating the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ API Key: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Lade .env aus dem env-Unterordner\n",
    "load_dotenv(dotenv_path=\"env/.env\")\n",
    "\n",
    "# Zugriff auf die Variable\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "print(\"GROQ API Key:\", groq_api_key)  # Testausgabe (nur zum Testen!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antwort der API: 200\n",
      "{'id': 'chatcmpl-46d45619-d366-48af-8f50-1b90dd08ab07', 'object': 'chat.completion', 'created': 1747872066, 'model': 'llama3-70b-8192', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Der wichtigste Faktor bei der Diagnose von Asthma ist die Anamnese, also die Erfassung der Krankheitsgeschichte und der Symptome durch Befragung des Patienten.'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'queue_time': 0.09352358699999999, 'prompt_tokens': 88, 'prompt_time': 0.002425165, 'completion_tokens': 46, 'completion_time': 0.131428571, 'total_tokens': 134, 'total_time': 0.133853736}, 'usage_breakdown': {'models': None}, 'system_fingerprint': 'fp_dd4ae1c591', 'x_groq': {'id': 'req_01jvtmpacre0d8kaq9k32cxk9q'}}\n",
      "Antwort des LLM:\n",
      "Der wichtigste Faktor bei der Diagnose von Asthma ist die Anamnese, also die Erfassung der Krankheitsgeschichte und der Symptome durch Befragung des Patienten.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 1. Frage formulieren\n",
    "query = \"Was ist der wichtigste Faktor bei der Diagnose von Asthma?\"\n",
    "\n",
    "# 2. LLM-Embedding-Modell laden\n",
    "model = SentenceTransformer(\"Sahajtomar/German-semantic\")\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "# 3. FAISS-Index und zugehörige Chunks laden\n",
    "index = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "\n",
    "# 4. Top-k ähnliche Chunks abrufen\n",
    "k = 5\n",
    "D, I = index.search(query_embedding, k)\n",
    "\n",
    "# Schutz gegen ungültige Indizes\n",
    "retrieved_chunks = [token_split_texts_2[i] for i in I[0] if i < len(token_split_texts_2)]\n",
    "\n",
    "# 5. Prompt konstruieren\n",
    "context = \"\\n\\n\".join(retrieved_chunks)\n",
    "prompt = f\"\"\"\n",
    "Du bist ein medizinischer Assistent. Beantworte die folgende Frage **nur** basierend auf dem gegebenen Kontext.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Frage: {query}\n",
    "Antwort:\n",
    "\"\"\"\n",
    "\n",
    "# 6. Anfrage an das Groq-LLM\n",
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "payload = {\n",
    "    \"model\": \"llama3-70b-8192\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher medizinischer Assistent auf Deutsch.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    \"temperature\": 0.3\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "# Troubleshooting\n",
    "print(\"Antwort der API:\", response.status_code)\n",
    "print(response.json())\n",
    "\n",
    "answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# 7. Ausgabe\n",
    "print(\"Antwort des LLM:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
